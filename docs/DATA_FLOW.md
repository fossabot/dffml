# Data Flow Facilitator

Define small operations that get run in something akin to an event loop on
steroids, where if there is a new permutation of inputs for a given operation
that new permutation gets run, It's nice because it creates a pipeline without
the user having to define the flow between operations, the flow is defined by
what data types enter the network. So as new inputs, which can be feed in or
generated by the output of an operation, enter the network, the set of
operations in the network is assessed to see which operations have input
parameters matching the data type of the input which just entered. If they do
we check if it'd be a new permutation, and then run it. That lets us right
small functional programming-esque "Operations" (functions or methods of a class)
which look like this:
https://github.com/intel/dffml/blob/master/feature/git/dffml_feature_git/feature/operations.py
So if we want to try compiling the project, we just write 15 different
functions that try to compile, make them take a git repo as an input, and the
orchestrator manages the locking on the git repo, then by throwing all 15 of
them into the network, one may come out as successful, returning to us data of
type compiled_codebase, and data of type
built_binary_with_debug_and_no_optimization, which floof operation would take
as an input.
